{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktr-A2EWRMYD"
      },
      "outputs": [],
      "source": [
        "#!rm -rf ./llama3.2-1b-finetuned\n",
        "#!rm -rf ./lora_weights\n",
        "#!pip uninstall -y torch torchvision torchaudio transformers datasets peft huggingface_hub tf-keras\n",
        "#!pip uninstall -y pandas scikit-learn openpyxl tqdm ipywidgets\n",
        "#!pip cache purge\n",
        "#!df -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXdRsyi0RR1I",
        "outputId": "17c59d62-b0b5-42d6-9838-bba41937f115"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!pip install datasets\\n!pip install transformers\\n!pip install tf-keras\\n!pip install peft\\n!pip install openpyxl\\n!pip install torch\\n!pip install pandas\\n!pip install huggingface_hub\\n!pip install scikit-learn\\n!pip install ipywidgets\\n!pip install tqdm'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install tf-keras\n",
        "!pip install peft\n",
        "!pip install openpyxl\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install huggingface_hub\n",
        "!pip install scikit-learn\n",
        "!pip install ipywidgets\n",
        "!pip install tqdm'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypnTjGP4RZ1P",
        "outputId": "e1815d7f-87b9-4ecd-ded4-7bd3c2103d4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754345018.633766    1059 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754345018.639147    1059 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754345018.652716    1059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754345018.652735    1059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754345018.652737    1059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754345018.652739    1059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from huggingface_hub import login\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgXUt4v4Rhd1"
      },
      "source": [
        "# Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD4wFOZ_ReWM"
      },
      "outputs": [],
      "source": [
        "def processing_data(file_path):\n",
        "\n",
        "  if not isinstance(file_path, str):\n",
        "\n",
        "    raise ValueError(\"Not a string.\")\n",
        "\n",
        "  # Reading Excel File\n",
        "  df_heart_dis = pd.read_excel(file_path)\n",
        "\n",
        "  # Removing Duplicate Rows\n",
        "  df_heart_dis = df_heart_dis.drop_duplicates()\n",
        "\n",
        "  # Removing Empty Rows\n",
        "  df_heart_dis = df_heart_dis.dropna()\n",
        "\n",
        "  # Removing Uncessary columns\n",
        "  columns_to_drop = [\"height\", \"weight\", \"age\", \"bp_category_encoded\", \"id\"]\n",
        "\n",
        "  df_heart_dis.drop(columns = columns_to_drop, inplace = True)\n",
        "\n",
        "  # Converting 2 = \"Male\" & 1 = \"Female\"\n",
        "  df_heart_dis[\"gender\"] = df_heart_dis[\"gender\"].apply(lambda x: \"Male\" if x == 2 else \"Female\")\n",
        "\n",
        "  # Converting 1 = \"Yes\" & 0 = \"No\"\n",
        "  df_heart_dis[\"cardio\"] = df_heart_dis[\"cardio\"].apply(lambda x: \"Yes\" if x == 1 else \"No\")\n",
        "\n",
        "  return df_heart_dis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebbfRthyRnt0",
        "outputId": "d3bbc49d-33e1-468c-f808-4edbc577a458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   gender  ap_hi  ap_lo  cholesterol  gluc  smoke  alco  active cardio  \\\n",
            "0    Male    110     80            1     1      0     0       1     No   \n",
            "1  Female    140     90            3     1      0     0       1    Yes   \n",
            "2  Female    130     70            3     1      0     0       0    Yes   \n",
            "3    Male    150    100            1     1      0     0       1    Yes   \n",
            "4  Female    100     60            1     1      0     0       0     No   \n",
            "\n",
            "   age_years        bmi           bp_category  \n",
            "0         50  21.967120  Hypertension Stage 1  \n",
            "1         55  34.927679  Hypertension Stage 2  \n",
            "2         51  23.507805  Hypertension Stage 1  \n",
            "3         48  28.710479  Hypertension Stage 2  \n",
            "4         47  23.011177                Normal  \n"
          ]
        }
      ],
      "source": [
        "df_heart_dis = processing_data(\"cardiovascular_dataset.xlsx\")\n",
        "\n",
        "print(df_heart_dis.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNZIEVtnRsQp"
      },
      "source": [
        "# Converting Data into a Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWKzRXCjRwYB"
      },
      "outputs": [],
      "source": [
        "def data_to_prompt (df_heart):\n",
        "\n",
        "    prompt = (\n",
        "        f'The patient is a {df_heart[\"age_years\"]} years old {df_heart[\"gender\"]}. '\n",
        "        f'BMI is {round(df_heart[\"bmi\"], 2)}. '\n",
        "        f'The patient {\"does\" if df_heart[\"smoke\"] == 1 else \"does not\"} smoke, '\n",
        "        f'{\"does\" if df_heart[\"alco\"] == 1 else \"does not\"} drink alcohol, and '\n",
        "        f'is physically {\"active\" if df_heart[\"active\"] == 1 else \"not active\"}. '\n",
        "        f'Blood pressure is {df_heart[\"ap_hi\"]} / {df_heart[\"ap_lo\"]}. '\n",
        "        f'Blood pressure category: {df_heart[\"bp_category\"]}. '\n",
        "    )\n",
        "\n",
        "    # Cholesterol\n",
        "    if df_heart[\"cholesterol\"] == 1:\n",
        "\n",
        "      prompt += \"Cholesterol is normal. \"\n",
        "\n",
        "    elif df_heart[\"cholesterol\"] == 2:\n",
        "\n",
        "      prompt += \"Cholesterol is above normal. \"\n",
        "\n",
        "    else:\n",
        "\n",
        "      prompt += \"Cholesterol is well above normal. \"\n",
        "\n",
        "    # Glucose\n",
        "    if df_heart[\"gluc\"] == 1:\n",
        "\n",
        "      prompt += \"Glucose is normal. \"\n",
        "\n",
        "    elif df_heart[\"gluc\"] == 2:\n",
        "\n",
        "      prompt += \"Glucose is above normal. \"\n",
        "\n",
        "    else:\n",
        "\n",
        "      prompt += \"Glucose is well above normal.\\n\"\n",
        "\n",
        "    prompt += \"Does the patient have heart disease? Answer:\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYEpuWzTSQhD",
        "outputId": "ee1ba206-cba6-4324-c3b8-17d98dada786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The patient is a 50 years old Male. BMI is 21.97. The patient does not smoke, does not drink alcohol, and is physically active. Blood pressure is 110 / 80. Blood pressure category: Hypertension Stage 1. Cholesterol is normal. Glucose is normal. Does the patient have heart disease? Answer:\n",
            "\n",
            "The patient is a 50 years old Male. BMI is 21.97. The patient does not smoke, does not drink alcohol, and is physically active. Blood pressure is 110 / 80. Blood pressure category: Hypertension Stage 1. Cholesterol is normal. Glucose is normal. Does the patient have heart disease? Answer: No\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_heart_dis[\"prompt\"] = df_heart_dis.apply(data_to_prompt, axis = 1) # just prompt\n",
        "df_heart_dis[\"prompt_ans\"] = df_heart_dis[\"prompt\"] + \" \" + df_heart_dis[\"cardio\"] # prompt + answer\n",
        "\n",
        "print(df_heart_dis[\"prompt\"][0] + \"\\n\")\n",
        "print(df_heart_dis[\"prompt_ans\"][0] + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGly3Rt1S-IX"
      },
      "source": [
        "# Training, Validation & Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDkc2s4pS72s",
        "outputId": "2cac504b-44e6-4560-c614-1f557c50d357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total: 68205 examples.\n",
            "Training: 47743 examples.\n",
            "Validation: 6821 examples.\n",
            "Testing: 13641 examples.\n",
            "\n",
            "{'prompt': 'The patient is a 55 years old Female. BMI is 28.57. The patient does not smoke, does not drink alcohol, and is physically active. Blood pressure is 130 / 80. Blood pressure category: Hypertension Stage 1. Cholesterol is normal. Glucose is normal. Does the patient have heart disease? Answer:', 'prompt_ans': 'The patient is a 55 years old Female. BMI is 28.57. The patient does not smoke, does not drink alcohol, and is physically active. Blood pressure is 130 / 80. Blood pressure category: Hypertension Stage 1. Cholesterol is normal. Glucose is normal. Does the patient have heart disease? Answer: No', 'cardio': 'No'} \n",
            "\n",
            "{'prompt': 'The patient is a 45 years old Female. BMI is 43.21. The patient does smoke, does drink alcohol, and is physically active. Blood pressure is 170 / 80. Blood pressure category: Hypertension Stage 1. Cholesterol is well above normal. Glucose is normal. Does the patient have heart disease? Answer:', 'prompt_ans': 'The patient is a 45 years old Female. BMI is 43.21. The patient does smoke, does drink alcohol, and is physically active. Blood pressure is 170 / 80. Blood pressure category: Hypertension Stage 1. Cholesterol is well above normal. Glucose is normal. Does the patient have heart disease? Answer: Yes', 'cardio': 'Yes'} \n",
            "\n",
            "{'prompt': 'The patient is a 59 years old Female. BMI is 33.79. The patient does not smoke, does not drink alcohol, and is physically active. Blood pressure is 130 / 80. Blood pressure category: Hypertension Stage 1. Cholesterol is normal. Glucose is normal. Does the patient have heart disease? Answer:', 'cardio': 'No'}\n"
          ]
        }
      ],
      "source": [
        "# Preparing dataset\n",
        "dataset = Dataset.from_pandas(df_heart_dis[[\"prompt\", \"prompt_ans\", \"cardio\"]])\n",
        "\n",
        "print(f'Total: {len(dataset)} examples.')\n",
        "\n",
        "# Splitting Data\n",
        "dataset = dataset.train_test_split(test_size = 0.2, seed = 42)\n",
        "train_val_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"] # 20% testing\n",
        "\n",
        "test_dataset = test_dataset.remove_columns([\"prompt_ans\"])# prompt and label needed only\n",
        "\n",
        "train_val_dataset = train_val_dataset.train_test_split(test_size = 0.125, seed = 42)\n",
        "train_dataset = train_val_dataset[\"train\"] # 70% training\n",
        "val_dataset = train_val_dataset[\"test\"] # 10% validation\n",
        "\n",
        "print(f'Training: {len(train_dataset)} examples.')\n",
        "print(f'Validation: {len(val_dataset)} examples.')\n",
        "print(f'Testing: {len(test_dataset)} examples.\\n')\n",
        "\n",
        "print(train_dataset[0], \"\\n\")\n",
        "print(val_dataset[0], \"\\n\")\n",
        "print(test_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7YUpaNSU6FG"
      },
      "source": [
        "# Loading Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO6NpdKmU5b1"
      },
      "outputs": [],
      "source": [
        "# Access Token from Hugging Face\n",
        "login(token = \"use_your_own\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk7qz6jeVJWH"
      },
      "outputs": [],
      "source": [
        "# Setting device\n",
        "device = \"cuda\"\n",
        "\n",
        "# Setting model\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16).to(device)\n",
        "\n",
        "# Setting tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-BqmvyEZ4d7"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG5XjTtp2Hiu"
      },
      "outputs": [],
      "source": [
        "def tokenizing_data(examples):\n",
        "\n",
        "    input_ids_batch = []\n",
        "    attention_mask_batch = []\n",
        "    label_ids_batch = []\n",
        "\n",
        "    #print(len(examples[\"prompt\"]))\n",
        "\n",
        "    for prompt, prompt_ans in zip(examples[\"prompt\"], examples[\"prompt_ans\"]):\n",
        "\n",
        "        tokenized_prompt = tokenizer(prompt, truncation = True, max_length = 100, padding = \"max_length\",\n",
        "                                     return_attention_mask = True)\n",
        "\n",
        "        tokenized_prompt_ans = tokenizer(prompt_ans, truncation = True, max_length = 100, padding = \"max_length\",\n",
        "                                         return_attention_mask = True)\n",
        "\n",
        "        # Find label start index (length of prompt input ids without padding)\n",
        "        label_start_idx = 0\n",
        "\n",
        "        for input_id in tokenized_prompt[\"input_ids\"]:\n",
        "\n",
        "            if input_id != tokenizer.pad_token_id:\n",
        "\n",
        "                label_start_idx += 1\n",
        "\n",
        "        # list of n -100\n",
        "        label_ids = [-100] * len(tokenized_prompt[\"input_ids\"])\n",
        "\n",
        "        # label (\"Yes\" or \"No\") token ids\n",
        "        label_ids[label_start_idx:] = tokenized_prompt_ans[\"input_ids\"][label_start_idx:]\n",
        "\n",
        "        input_ids_batch.append(tokenized_prompt_ans[\"input_ids\"])\n",
        "        attention_mask_batch.append(tokenized_prompt_ans[\"attention_mask\"])\n",
        "        label_ids_batch.append(label_ids)\n",
        "\n",
        "    return {\"input_ids\": input_ids_batch, \"attention_mask\": attention_mask_batch, \"label_ids\": label_ids_batch,}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r8LnuFxejVk",
        "outputId": "5b832224-a163-4b7b-adf8-431d474bb8fd",
        "scrolled": true,
        "colab": {
          "referenced_widgets": [
            "26efe0e33750411c9d7960d166321301",
            "99ab37bfe0f048b6a791e8f70457d711"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26efe0e33750411c9d7960d166321301",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/47743 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99ab37bfe0f048b6a791e8f70457d711",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6821 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prompt': 'The patient is a 55 years old Female. BMI is 28.57. The patient does not smoke, does not drink alcohol, and is physically active. Blood pressure is 130 / 80. Blood pressure category: Hypertension Stage 1. Cholesterol is normal. Glucose is normal. Does the patient have heart disease? Answer:', 'prompt_ans': 'The patient is a 55 years old Female. BMI is 28.57. The patient does not smoke, does not drink alcohol, and is physically active. Blood pressure is 130 / 80. Blood pressure category: Hypertension Stage 1. Cholesterol is normal. Glucose is normal. Does the patient have heart disease? Answer: No', 'cardio': 'No', 'input_ids': [128000, 791, 8893, 374, 264, 220, 2131, 1667, 2362, 29738, 13, 47224, 374, 220, 1591, 13, 3226, 13, 578, 8893, 1587, 539, 16603, 11, 1587, 539, 7172, 13200, 11, 323, 374, 22655, 4642, 13, 20671, 7410, 374, 220, 5894, 611, 220, 1490, 13, 20671, 7410, 5699, 25, 39515, 531, 2711, 22891, 220, 16, 13, 921, 35244, 374, 4725, 13, 8444, 94697, 374, 4725, 13, 12838, 279, 8893, 617, 4851, 8624, 30, 22559, 25, 2360, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label_ids': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2360, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001]}\n"
          ]
        }
      ],
      "source": [
        "# Tokenizing dataset\n",
        "batch_size = 32\n",
        "trainset_tokenized = train_dataset.map(tokenizing_data, batched = True, batch_size = batch_size)\n",
        "valset_tokenized = val_dataset.map(tokenizing_data, batched = True, batch_size = batch_size)\n",
        "\n",
        "print(trainset_tokenized[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Nqg4Y04Z4V"
      },
      "source": [
        "# Training Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSjlM5mt2Hiv",
        "outputId": "b592a9f4-e10c-4ffc-d98f-cbf9c3de0ae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
          ]
        }
      ],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r = 16, # number of weights per input and output neurons\n",
        "    lora_alpha = 32, # scaling factor for LoRa update\n",
        "    target_modules = [\"q_proj\", \"v_proj\"], # modules to apply LoRa\n",
        "    task_type = TaskType.CAUSAL_LM, # specific task\n",
        "    lora_dropout = 0.1, # dropout rate applied to LoRa weight matrices\n",
        "    bias = \"none\" # no biases are trained\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config) # adds LoRa weight matrixes to the original model\n",
        "model.print_trainable_parameters() # prints number of parameters in the model are trainable vs total parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpKN-mLw2Hiv"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./llama3.2-1b-finetuned\",\n",
        "    per_device_train_batch_size = 8, # number of examples in one batch processed at a time\n",
        "    gradient_accumulation_steps = 4, # backward pass after n batches\n",
        "    num_train_epochs = 10,\n",
        "    learning_rate = 2e-4,\n",
        "    bf16 = True,\n",
        "    logging_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    eval_strategy = \"epoch\",\n",
        "    report_to = \"none\",\n",
        "    label_names = [\"label_ids\"],\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer = tokenizer,\n",
        "    mlm = False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model, # model\n",
        "    args = training_args, # training params\n",
        "    train_dataset = trainset_tokenized,\n",
        "    eval_dataset = valset_tokenized,\n",
        "    data_collator = data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1alvZmcd4e0i",
        "outputId": "32b1493c-9471-4bdc-b257-ab0cba577ccc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14920' max='14920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14920/14920 1:08:46, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.253200</td>\n",
              "      <td>0.228147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.223200</td>\n",
              "      <td>0.226157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.221200</td>\n",
              "      <td>0.224255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.220100</td>\n",
              "      <td>0.223000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.219200</td>\n",
              "      <td>0.222388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.218500</td>\n",
              "      <td>0.222090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.217900</td>\n",
              "      <td>0.221586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.217300</td>\n",
              "      <td>0.221211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.216700</td>\n",
              "      <td>0.220818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.216100</td>\n",
              "      <td>0.220643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14920, training_loss=0.22234243234424744, metrics={'train_runtime': 4127.323, 'train_samples_per_second': 115.675, 'train_steps_per_second': 3.615, 'total_flos': 2.79253595049984e+17, 'train_loss': 0.22234243234424744, 'epoch': 10.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T-w2QwT2Hiv"
      },
      "outputs": [],
      "source": [
        "# Saving LoRa adapter weights into a file\n",
        "model.save_pretrained(\"lora_weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvM9fEUD4u19"
      },
      "source": [
        "# Evaluating Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utW8O8iY2Hiv",
        "outputId": "ae6e81d8-fe04-4eba-877e-2250c84297c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# Setting device\\ndevice = \"cuda\"\\n\\n# Setting model\\nmodel_id = \"meta-llama/Llama-3.2-1B\"\\n\\nbase_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16).to(device)\\n\\ntrained_model = PeftModel.from_pretrained(base_model, \"lora_weights\")\\n\\n# Setting tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)\\ntokenizer.pad_token = tokenizer.eos_token'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading Model with trained LoRa adapter weights\n",
        "'''\n",
        "# Setting device\n",
        "device = \"cuda\"\n",
        "\n",
        "# Setting model\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16).to(device)\n",
        "\n",
        "trained_model = PeftModel.from_pretrained(base_model, \"lora_weights\")\n",
        "\n",
        "# Setting tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)\n",
        "tokenizer.pad_token = tokenizer.eos_token'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYBsGMRS2Hiv"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(true_labels, pred_labels):\n",
        "\n",
        "  tp, fp = 0, 0\n",
        "  tn, fn = 0, 0\n",
        "\n",
        "  for true, pred in zip(true_labels, pred_labels):\n",
        "\n",
        "    if true == \"Yes\" and pred == \"Yes\":\n",
        "\n",
        "      tp += 1\n",
        "\n",
        "    elif true == \"No\" and pred == \"No\":\n",
        "\n",
        "      tn += 1\n",
        "\n",
        "    elif true == \"No\" and pred == \"Yes\":\n",
        "\n",
        "      fp += 1\n",
        "\n",
        "    else:\n",
        "\n",
        "      fn += 1\n",
        "\n",
        "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "  precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "  recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "  f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "  return accuracy, precision, recall, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vSnrCqj2Hiv"
      },
      "outputs": [],
      "source": [
        "def eval(model, tokenizer, dataset):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # To avoid warnings during inference\n",
        "    model.generation_config.temperature = None\n",
        "    model.generation_config.top_p = None\n",
        "\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "\n",
        "    for example in dataset:\n",
        "\n",
        "        true_label = example[\"cardio\"]\n",
        "\n",
        "        prompt = example[\"prompt\"]\n",
        "\n",
        "        # Tokenize the prompt\n",
        "        inputs = tokenizer(prompt, return_tensors = \"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = model.generate(\n",
        "                input_ids = inputs[\"input_ids\"],\n",
        "                attention_mask = inputs[\"attention_mask\"],\n",
        "                max_new_tokens = 2, # generate only \"Yes\" or \"No\"\n",
        "                do_sample = False, # Greedy decoding (most likely next token)\n",
        "                pad_token_id = tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        # Decode and extract the answer\n",
        "        decoded = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
        "        pred = decoded.replace(prompt, \"\").strip().rstrip('.') # get first word after prompt\n",
        "\n",
        "        #print(decoded)\n",
        "        #print(pred)\n",
        "\n",
        "        preds.append(pred)\n",
        "        true_labels.append(true_label)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy, precision, recall, f1_score = compute_metrics(true_labels, preds)\n",
        "\n",
        "    print(\"Accuracy:{}\".format(accuracy * 100))\n",
        "    print(\"Precision:{}\".format(precision))\n",
        "    print(\"Recall:{}\".format(recall))\n",
        "    print(\"F1-Score:{}\".format(f1_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skvuf6yj2Hiv",
        "scrolled": true,
        "outputId": "e90b43c1-4ee5-4449-8c7f-e231a7a3207f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------Training Dataset------\n",
            "Accuracy:73.7699767505184\n",
            "Precision:0.771652761223679\n",
            "Recall:0.662234155905042\n",
            "F1-Score:0.7127686414826028\n",
            "\n",
            "------Validation Dataset------\n",
            "Accuracy:73.12710746224894\n",
            "Precision:0.7663299663299663\n",
            "Recall:0.6664714494875549\n",
            "F1-Score:0.7129209083790131\n",
            "\n",
            "------Testing Dataset------\n",
            "Accuracy:73.22776922513012\n",
            "Precision:0.7737887765772046\n",
            "Recall:0.6535178098322049\n",
            "F1-Score:0.7085860197893392\n"
          ]
        }
      ],
      "source": [
        "print(\"------Training Dataset------\")\n",
        "eval(model, tokenizer, train_dataset)\n",
        "print(\"\\n------Validation Dataset------\")\n",
        "eval(model, tokenizer, val_dataset)\n",
        "print(\"\\n------Testing Dataset------\")\n",
        "eval(model, tokenizer, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86DXmSuplxtJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}